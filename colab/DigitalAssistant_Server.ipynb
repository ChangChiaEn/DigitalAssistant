{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Digital Assistant - AI Server\n",
    "\n",
    "This notebook runs the LLM backend for your Desktop Digital Assistant.\n",
    "\n",
    "**Steps:**\n",
    "1. Run all cells (Runtime → Run all)\n",
    "2. Copy the `trycloudflare.com` URL printed at the bottom\n",
    "3. Paste it into your Desktop App settings\n",
    "\n",
    "**Model:** Qwen2.5-14B-Instruct\n",
    "\n",
    "**Tunnel:** Cloudflare (free, no account needed)\n",
    "\n",
    "**Requirements:** Colab Pro with GPU (A100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Install dependencies\n",
    "!pip install -q transformers accelerate torch fastapi uvicorn sentencepiece nest-asyncio\n",
    "!wget -q https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64 -O /usr/local/bin/cloudflared\n",
    "!chmod +x /usr/local/bin/cloudflared\n",
    "print(\"✅ Dependencies installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Load Model\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "MODEL_ID = \"Qwen/Qwen2.5-14B-Instruct\"\n",
    "\n",
    "print(f\"Loading {MODEL_ID}...\")\n",
    "print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "print(\"✅ Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Define the AI Chat Engine\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"你是一個桌面數位助理，運行在使用者的 Windows 電腦上。\n",
    "你可以幫助使用者完成各種任務。\n",
    "\n",
    "可用技能:\n",
    "{skills}\n",
    "\n",
    "重要規則:\n",
    "1. 永遠用繁體中文回答\n",
    "2. 回覆必須是合法 JSON 格式\n",
    "3. 如果需要執行技能，回傳:\n",
    "   {{\"text\": \"說明文字\", \"skill\": \"技能名稱\", \"args\": {{參數}}}}\n",
    "4. 如果只是聊天，回傳:\n",
    "   {{\"text\": \"回答內容\", \"skill\": \"\", \"args\": {{}}}}\n",
    "5. 回答要簡潔（除非使用者要求詳細）\n",
    "6. 當使用者要求建立文件（PPT/Word/Excel），你必須自己生成完整、專業、詳細的內容。絕對不要反問使用者要什麼內容，直接根據主題產生。\n",
    "7. 建立PPT時，根據主題選擇最適合的 theme:\n",
    "   - dark: 科技、AI、程式、未來相關\n",
    "   - corporate: 商業報告、公司簡報、財務\n",
    "   - nature: 環保、生態、農業、健康\n",
    "   - warm: 教育、文化、歷史、藝術\n",
    "   - ocean: 海洋、旅遊、地理、運動\n",
    "   - minimal: 簡約、設計、建築\n",
    "8. 建立Word時，使用 # ## ### 標記標題層級，用 - 開頭標記要點。生成結構完整的專業文件。\n",
    "9. PPT 至少要生成 5 張投影片，每張內容要有 3-5 個要點。\n",
    "10. **搜尋並製作簡報工作流程（非常重要）**：\n",
    "    - **關鍵區別**：\n",
    "      * `search_web`: 只是打開瀏覽器搜尋頁面，不會返回內容給 AI，無法用於製作簡報\n",
    "      * `fetch_news`: 真正搜尋並返回內容摘要，可以用於製作簡報\n",
    "    - 當使用者要求「搜尋XX新聞然後做成簡報」、「找XX資料做成簡報」等需求時：\n",
    "      * **必須使用 fetch_news**，絕對不要使用 search_web\n",
    "      * 先執行 fetch_news 技能搜尋資料（根據需求判斷 max_results，通常 5-10）\n",
    "      * 等待搜尋結果返回後，**自動分析使用者原始需求**，判斷是否需要製作簡報\n",
    "      * 如果使用者要求製作簡報/文件，**在下一輪回應中自動執行 create_ppt/create_docx**\n",
    "      * 不要等待使用者再次確認，直接根據原始需求完成整個工作流程\n",
    "      * 簡報內容必須基於搜尋結果，不要編造資料\n",
    "      * 每張投影片要引用資料來源（如果 fetch_news 有提供來源連結）\n",
    "    - 範例：使用者說「幫我搜尋最新的AI新聞然後做成簡報」\n",
    "      * 第一輪：執行 fetch_news(\"最新的AI新聞\", 8)\n",
    "      * 第二輪（自動）：看到搜尋結果後，立即執行 create_ppt，根據搜尋結果製作簡報\n",
    "      * 不要只回覆「已搜尋」，要完成整個工作流程\n",
    "\n",
    "範例:\n",
    "- 使用者: \"幫我做一份關於AI的簡報\"\n",
    "  回覆: {{\"text\": \"好的，已為你建立AI簡報\", \"skill\": \"create_ppt\", \"args\": {{\"title\": \"人工智慧簡介\", \"theme\": \"dark\", \"slides_json\": [{{\"title\": \"什麼是AI\", \"content\": \"人工智慧是模擬人類智慧的技術\\\\n機器學習讓電腦從資料中自動學習\\\\n深度學習模仿人腦神經網路結構\\\\n自然語言處理讓機器理解人類語言\"}}, {{\"title\": \"AI的應用\", \"content\": \"醫療: AI輔助診斷與藥物開發\\\\n金融: 風險評估與詐欺偵測\\\\n教育: 個人化學習與智能tutoring\\\\n交通: 自動駕駛與路線優化\"}}]}}}}\n",
    "- 使用者: \"幫我寫一份環保報告\"\n",
    "  回覆: {{\"text\": \"好的，已建立環保報告\", \"skill\": \"create_docx\", \"args\": {{\"title\": \"環境保護報告\", \"content\": \"# 前言\\\\n本報告探討當前環境問題與解決方案。\\\\n\\\\n## 現況分析\\\\n- 全球平均溫度持續上升\\\\n- 極端氣候事件頻率增加\\\\n...\"}}}}\n",
    "- 使用者: \"幫我搜尋最新的AI新聞然後做成簡報\"\n",
    "  回覆: {{\"text\": \"正在搜尋最新的AI新聞...\", \"skill\": \"fetch_news\", \"args\": {{\"query\": \"最新的AI新聞\", \"max_results\": 8}}}}\n",
    "  （注意：這需要兩步驟，先 fetch_news 取得資料，然後在下一輪對話中根據搜尋結果執行 create_ppt）\n",
    "\"\"\"\n",
    "\n",
    "def generate_response(messages, skills=None):\n",
    "    \"\"\"Generate a response from the model.\"\"\"\n",
    "    skills_text = \"\"\n",
    "    if skills:\n",
    "        skills_text = \"\\n\".join(\n",
    "            f\"- {s['name']}: {s['description']} (params: {s.get('params', {})})\"\n",
    "            for s in skills\n",
    "        )\n",
    "\n",
    "    system_msg = SYSTEM_PROMPT.format(skills=skills_text or \"(無可用技能)\")\n",
    "\n",
    "    conversation = [{\"role\": \"system\", \"content\": system_msg}]\n",
    "    for msg in messages[-20:]:\n",
    "        conversation.append({\n",
    "            \"role\": msg.get(\"role\", \"user\"),\n",
    "            \"content\": msg.get(\"content\", \"\")\n",
    "        })\n",
    "\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        conversation,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=2048,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            repetition_penalty=1.1,\n",
    "            do_sample=True\n",
    "        )\n",
    "\n",
    "    new_tokens = outputs[0][inputs['input_ids'].shape[1]:]\n",
    "    response = tokenizer.decode(new_tokens, skip_special_tokens=True)\n",
    "\n",
    "    return response.strip()\n",
    "\n",
    "# Quick test\n",
    "test = generate_response([{\"role\": \"user\", \"content\": \"你好\"}])\n",
    "print(f\"Test response: {test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: FastAPI Server + Cloudflare Tunnel (free, no account needed)\n",
    "import json\n",
    "import re\n",
    "import subprocess\n",
    "import threading\n",
    "import time\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "from fastapi import FastAPI, Request\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "import uvicorn\n",
    "\n",
    "app = FastAPI(title=\"Digital Assistant AI Server\")\n",
    "\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=[\"*\"],\n",
    "    allow_methods=[\"*\"],\n",
    "    allow_headers=[\"*\"],\n",
    ")\n",
    "\n",
    "@app.get(\"/health\")\n",
    "async def health():\n",
    "    return {\n",
    "        \"status\": \"ok\",\n",
    "        \"model\": \"Qwen2.5-14B-Instruct\",\n",
    "        \"gpu\": torch.cuda.get_device_name(0),\n",
    "    }\n",
    "\n",
    "def parse_ai_response(raw_response):\n",
    "    \"\"\"Parse the AI response, handling nested JSON with skill/args.\"\"\"\n",
    "    # Try parsing the entire response as JSON\n",
    "    try:\n",
    "        parsed = json.loads(raw_response)\n",
    "        if isinstance(parsed, dict) and \"text\" in parsed:\n",
    "            return parsed\n",
    "    except json.JSONDecodeError:\n",
    "        pass\n",
    "\n",
    "    # Try to find JSON by matching braces (supports nested objects)\n",
    "    depth = 0\n",
    "    start = -1\n",
    "    for i, ch in enumerate(raw_response):\n",
    "        if ch == '{':\n",
    "            if depth == 0:\n",
    "                start = i\n",
    "            depth += 1\n",
    "        elif ch == '}':\n",
    "            depth -= 1\n",
    "            if depth == 0 and start >= 0:\n",
    "                candidate = raw_response[start:i+1]\n",
    "                try:\n",
    "                    parsed = json.loads(candidate)\n",
    "                    if isinstance(parsed, dict) and \"text\" in parsed:\n",
    "                        return parsed\n",
    "                except json.JSONDecodeError:\n",
    "                    continue\n",
    "\n",
    "    # Fallback: return as plain text\n",
    "    return {\"text\": raw_response, \"skill\": \"\", \"args\": {}}\n",
    "\n",
    "@app.post(\"/chat\")\n",
    "async def chat(request: Request):\n",
    "    body = await request.json()\n",
    "    messages = body.get(\"messages\", [])\n",
    "    skills = body.get(\"skills\", [])\n",
    "\n",
    "    raw_response = generate_response(messages, skills)\n",
    "    return parse_ai_response(raw_response)\n",
    "\n",
    "# --- Start Cloudflare Tunnel (free, no signup) ---\n",
    "def start_cloudflared():\n",
    "    process = subprocess.Popen(\n",
    "        [\"cloudflared\", \"tunnel\", \"--url\", \"http://localhost:8000\"],\n",
    "        stdout=subprocess.PIPE,\n",
    "        stderr=subprocess.PIPE,\n",
    "        text=True\n",
    "    )\n",
    "    for line in process.stderr:\n",
    "        if \"trycloudflare.com\" in line:\n",
    "            import re as _re\n",
    "            match = _re.search(r'(https://[^\\s]+trycloudflare\\.com)', line)\n",
    "            if match:\n",
    "                print(\"=\" * 60)\n",
    "                print(\"AI Server is running!\")\n",
    "                print(\"\")\n",
    "                print(\"Copy this URL to your Desktop App settings:\")\n",
    "                print(\"\")\n",
    "                print(f\"   {match.group(1)}\")\n",
    "                print(\"\")\n",
    "                print(f\"Model: Qwen2.5-14B-Instruct\")\n",
    "                print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "                print(\"=\" * 60)\n",
    "\n",
    "tunnel_thread = threading.Thread(target=start_cloudflared, daemon=True)\n",
    "tunnel_thread.start()\n",
    "\n",
    "time.sleep(2)\n",
    "print(\"Starting server... tunnel URL will appear shortly:\")\n",
    "\n",
    "import asyncio\n",
    "config = uvicorn.Config(app, host=\"0.0.0.0\", port=8000)\n",
    "server = uvicorn.Server(config)\n",
    "loop = asyncio.get_event_loop()\n",
    "loop.run_until_complete(server.serve())"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
